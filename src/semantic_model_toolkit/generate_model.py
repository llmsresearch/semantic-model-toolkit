import os
from datetime import datetime

from typing import List, Optional, Dict

from loguru import logger
from snowflake.connector import SnowflakeConnection

from semantic_model_toolkit.core.config import Config, LLMConfig
from semantic_model_toolkit.data_processing import data_types, proto_utils
from semantic_model_toolkit.llm_utils import get_llm_client
from semantic_model_toolkit.protos import semantic_model_pb2
from semantic_model_toolkit.snowflake_utils.snowflake_connector import (
    AUTOGEN_TOKEN,
    DIMENSION_DATATYPES,
    MEASURE_DATATYPES,
    OBJECT_DATATYPES,
    TIME_MEASURE_DATATYPES,
    get_table_representation,
    get_valid_schemas_tables_columns_df,
    get_table_foreign_keys,
)
from semantic_model_toolkit.snowflake_utils.utils import create_fqn_table
from semantic_model_toolkit.validate.context_length import validate_context_length

_PLACEHOLDER_COMMENT = "  "
_FILL_OUT_TOKEN = " # <FILL-OUT>"
# TODO add _AUTO_GEN_TOKEN to the end of the auto generated descriptions.
_AUTOGEN_COMMENT_TOKEN = "__"
_DEFAULT_N_SAMPLE_VALUES_PER_COL = 3
_AUTOGEN_COMMENT_WARNING = f"# NOTE: This file was auto-generated by the semantic model toolkit. Please fill out placeholders marked with {_FILL_OUT_TOKEN} (or remove if not relevant) and verify autogenerated comments.\n"


def _get_placeholder_filter() -> List[semantic_model_pb2.NamedFilter]:
    return [
        semantic_model_pb2.NamedFilter(
            name=_PLACEHOLDER_COMMENT,
            synonyms=[_PLACEHOLDER_COMMENT],
            description=_PLACEHOLDER_COMMENT,
            expr=_PLACEHOLDER_COMMENT,
        )
    ]


def _get_placeholder_joins() -> List[semantic_model_pb2.Relationship]:
    return [
        semantic_model_pb2.Relationship(
            name=_PLACEHOLDER_COMMENT,
            left_table=_PLACEHOLDER_COMMENT,
            right_table=_PLACEHOLDER_COMMENT,
            join_type=semantic_model_pb2.JoinType.inner,
            relationship_columns=[
                semantic_model_pb2.RelationKey(
                    left_column=_PLACEHOLDER_COMMENT,
                    right_column=_PLACEHOLDER_COMMENT,
                )
            ],
            relationship_type=semantic_model_pb2.RelationshipType.many_to_one,
        )
    ]


def _generate_relationships_from_foreign_keys(table_objects: List[semantic_model_pb2.Table], conn: SnowflakeConnection) -> List[semantic_model_pb2.Relationship]:
    """
    Generate relationships between tables based on detected foreign key constraints.
    
    Args:
        table_objects: List of semantic model table objects
        conn: Snowflake connection
        
    Returns:
        List of relationship objects representing the joins between tables
    """
    from semantic_model_toolkit.snowflake_utils.snowflake_connector import get_table_foreign_keys
    
    relationships = []
    table_map = {table.name: table for table in table_objects}
    
    # For each table, check for foreign keys
    for table in table_objects:
        try:
            # Get fully qualified table name
            table_fqn = f"{table.base_table.database}.{table.base_table.schema}.{table.base_table.table}"
            
            # Get foreign keys for this table
            try:
                foreign_keys = get_table_foreign_keys(conn, table_fqn)
                
                if not foreign_keys:
                    logger.info(f"No foreign keys found for table {table_fqn}")
                    continue
                    
                for fk in foreign_keys:
                    try:
                        # Check if this is a useful foreign key relationship
                        # The primary key table should be in our model (referenced_table)
                        pk_table_name = fk.get('referenced_table', '')
                        pk_column = fk.get('referenced_column', '')
                        fk_table_name = fk.get('foreign_key_table', '')
                        fk_column = fk.get('column_name', '')
                        fk_database = fk.get('referenced_database', '')
                        fk_schema = fk.get('referenced_schema', '')
                        constraint_name = fk.get('constraint_name', '')
                        
                        # Skip if no primary key table or column
                        if not pk_table_name or not pk_column:
                            logger.warning(f"Missing primary key information for foreign key in {table_fqn}")
                            continue
                        
                        # Skip if this refers to the current table (skip self-references)
                        if pk_table_name == table.base_table.table and fk_table_name == table.base_table.table:
                            logger.info(f"Skipping self-reference in table {table.name}")
                            continue
                            
                        # Find the tables in our model
                        pk_table_obj = None
                        for t in table_objects:
                            if t.base_table.table == pk_table_name:
                                pk_table_obj = t
                                break
                                
                        if not pk_table_obj:
                            logger.info(f"Primary key table {pk_table_name} not in model, skipping relationship")
                            continue
                        
                        # Create a relationship name if missing
                        rel_name = constraint_name
                        if not rel_name:
                            rel_name = f"FK_{table.name}_{pk_table_obj.name}"
                        
                        # Create relationship
                        relationship = semantic_model_pb2.Relationship(
                            name=rel_name,
                            left_table=table.name,
                            right_table=pk_table_obj.name,
                            join_type=semantic_model_pb2.JoinType.inner,  # Default to inner join
                            relationship_columns=[
                                semantic_model_pb2.RelationKey(
                                    left_column=fk_column,
                                    right_column=pk_column
                                )
                            ],
                            relationship_type=semantic_model_pb2.RelationshipType.many_to_one,  # Default to many-to-one
                        )
                        relationships.append(relationship)
                        logger.info(f"Created relationship: {rel_name} between {table.name} and {pk_table_obj.name}")
                    except Exception as e:
                        logger.warning(f"Error processing foreign key {fk} for table {table_fqn}: {str(e)}")
            except Exception as e:
                logger.warning(f"Error detecting foreign keys for table {table_fqn}: {str(e)}")
        except Exception as e:
            logger.warning(f"Error processing table {table.name}: {str(e)}")
    
    # If no relationships were found, return placeholder joins
    if not relationships:
        logger.warning("No relationships detected from foreign keys. Using placeholder joins.")
        return _get_placeholder_joins()
        
    return relationships


def generate_description_with_llm(context: str, entity_type: str, entity_name: str, conn: SnowflakeConnection, llm_config: Optional[LLMConfig] = None) -> str:
    """
    Generate a description for an entity using an LLM.
    
    Args:
        context: Contextual information about the entity
        entity_type: Type of entity (table, column, etc.)
        entity_name: Name of the entity
        conn: Snowflake connection (for Cortex LLM)
        llm_config: LLM configuration
        
    Returns:
        Generated description
    """
    if not llm_config:
        return _PLACEHOLDER_COMMENT
    
    # Create the LLM client based on configuration
    try:
        llm_client = get_llm_client(llm_config)
        
        # Connect the Snowflake connection to the Cortex LLM client if applicable
        if llm_config.provider == "cortex" and hasattr(llm_client, "connect"):
            llm_client.connect(conn)
        
        # Create a prompt for the LLM
        prompt = f"Generate a clear, concise description for a {entity_type} named '{entity_name}' in a Snowflake semantic model. Consider the data type, values, and context provided below. Keep the description factual and brief (1-2 sentences)."
        
        # Generate description
        description = llm_client.generate_description(context, prompt)
        return description + _AUTOGEN_COMMENT_TOKEN
    except Exception as e:
        logger.error(f"Error generating description with LLM: {e}")
        return _PLACEHOLDER_COMMENT


def generate_synonyms_with_llm(context: str, entity_type: str, entity_name: str, conn: SnowflakeConnection, llm_config: Optional[LLMConfig] = None) -> List[str]:
    """
    Generate synonyms for an entity using an LLM.
    
    Args:
        context: Contextual information about the entity
        entity_type: Type of entity (table, column, etc.)
        entity_name: Name of the entity
        conn: Snowflake connection (for Cortex LLM)
        llm_config: LLM configuration
        
    Returns:
        List of generated synonyms
    """
    if not llm_config:
        return [_PLACEHOLDER_COMMENT]
    
    try:
        llm_client = get_llm_client(llm_config)
        
        # Connect the Snowflake connection to the Cortex LLM client if applicable
        if llm_config.provider == "cortex" and hasattr(llm_client, "connect"):
            llm_client.connect(conn)
        
        # Create a prompt for the LLM
        prompt = f"Generate 5-8 alternative names or synonyms for a {entity_type} named '{entity_name}' in a data model. Consider the context below and provide business-friendly terms. Return only the synonyms separated by commas, without any explanations or additional text."
        
        # Generate synonyms
        synonyms_str = llm_client.generate_description(context, prompt)
        synonyms = [s.strip() for s in synonyms_str.split(',')]
        return [s for s in synonyms if s and s != entity_name]
    except Exception as e:
        logger.error(f"Error generating synonyms with LLM: {e}")
        return [_PLACEHOLDER_COMMENT]


def _raw_table_to_semantic_context_table(
    database: str, schema: str, raw_table: data_types.Table, conn: SnowflakeConnection, llm_config: Optional[LLMConfig] = None
) -> semantic_model_pb2.Table:
    """
    Converts a raw table representation to a semantic model table in protobuf format.

    Args:
        database (str): The name of the database containing the table.
        schema (str): The name of the schema containing the table.
        raw_table (data_types.Table): The raw table object to be transformed.
        conn (SnowflakeConnection): Snowflake connection to use for Cortex LLM.
        llm_config (Optional[LLMConfig]): LLM configuration for generating descriptions

    Returns:
        semantic_model_pb2.Table: A protobuf representation of the semantic table.

    This function categorizes table columns into TimeDimensions, Dimensions, or Measures based on their data type,
    populates them with sample values, and sets placeholders for descriptions and filters.
    """

    # For each column, decide if it is a TimeDimension, Measure, or Dimension column.
    # For now, we decide this based on datatype.
    # Any time datatype, is TimeDimension.
    # Any varchar/text is Dimension.
    # Any numerical column is Measure.

    time_dimensions = []
    dimensions = []
    measures = []

    # Prepare column context for better LLM generation
    def _prepare_column_context(col, raw_table_name):
        sample_values_str = ', '.join(str(v) for v in col.values[:5]) if col.values else 'No sample values'
        return f"""
Table: {raw_table_name}
Column: {col.column_name}
Data Type: {col.column_type}
Sample Values: {sample_values_str}
"""

    for col in raw_table.columns:
        # Prepare enhanced context for LLM description generation
        column_context = _prepare_column_context(col, raw_table.name)
        
        if col.column_type.upper() in TIME_MEASURE_DATATYPES:
            description = col.comment if col.comment else generate_description_with_llm(
                column_context, "time dimension column", col.column_name, conn, llm_config
            )
            synonyms = generate_synonyms_with_llm(
                column_context, "time dimension column", col.column_name, conn, llm_config
            )
            
            time_dimensions.append(
                semantic_model_pb2.TimeDimension(
                    name=col.column_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=synonyms,
                    description=description,
                )
            )

        elif col.column_type.upper() in DIMENSION_DATATYPES:
            description = col.comment if col.comment else generate_description_with_llm(
                column_context, "dimension column", col.column_name, conn, llm_config
            )
            synonyms = generate_synonyms_with_llm(
                column_context, "dimension column", col.column_name, conn, llm_config
            )
            
            dimensions.append(
                semantic_model_pb2.Dimension(
                    name=col.column_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=synonyms,
                    description=description,
                )
            )

        elif col.column_type.upper() in MEASURE_DATATYPES:
            description = col.comment if col.comment else generate_description_with_llm(
                column_context, "measure column", col.column_name, conn, llm_config
            )
            synonyms = generate_synonyms_with_llm(
                column_context, "measure column", col.column_name, conn, llm_config
            )
            
            measures.append(
                semantic_model_pb2.Fact(
                    name=col.column_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=synonyms,
                    description=description,
                )
            )
        elif col.column_type.upper() in OBJECT_DATATYPES:
            logger.warning(
                f"""We don't currently support {col.column_type} as an input column datatype to the Semantic Model. We are skipping column {col.column_name} for now."""
            )
            continue
        else:
            logger.warning(
                f"Column datatype does not map to a known datatype. Input was = {col.column_type}. We are going to place as a Dimension for now."
            )
            
            description = col.comment if col.comment else generate_description_with_llm(
                column_context, "general column", col.column_name, conn, llm_config
            )
            synonyms = generate_synonyms_with_llm(
                column_context, "general column", col.column_name, conn, llm_config
            )
            
            dimensions.append(
                semantic_model_pb2.Dimension(
                    name=col.column_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=synonyms,
                    description=description,
                )
            )
    if len(time_dimensions) + len(dimensions) + len(measures) == 0:
        raise ValueError(
            f"No valid columns found for table {raw_table.name}. Please verify that this table contains column's datatypes not in {OBJECT_DATATYPES}."
        )

    # Create context for table description
    table_context = f"Database: {database}\nSchema: {schema}\nTable: {raw_table.name}\nColumns: {', '.join(col.column_name for col in raw_table.columns)}\nColumn Types: {', '.join(f'{col.column_name} ({col.column_type})' for col in raw_table.columns[:5])}"
    
    table_description = raw_table.comment if raw_table.comment else generate_description_with_llm(
        table_context, "table", raw_table.name, conn, llm_config
    )

    return semantic_model_pb2.Table(
        name=raw_table.name,
        base_table=semantic_model_pb2.FullyQualifiedTable(
            database=database, schema=schema, table=raw_table.name
        ),
        # For fields we can not automatically infer, leave a comment for the user to fill out.
        description=table_description,
        dimensions=dimensions,
        time_dimensions=time_dimensions,
        measures=measures,
    )


def raw_schema_to_semantic_context(
    base_tables: List[str],
    semantic_model_name: str,
    conn: SnowflakeConnection,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    allow_joins: Optional[bool] = False,
    llm_config: Optional[LLMConfig] = None,
) -> semantic_model_pb2.SemanticModel:
    """
    Converts a list of fully qualified Snowflake table names into a semantic model.

    Parameters:
    - base_tables  (list[str]): Fully qualified table names to include in the semantic model.
    - semantic_model_name (str): A meaningful semantic model name.
    - conn (SnowflakeConnection): SnowflakeConnection to reuse.
    - n_sample_values (int): The number of sample values per col.
    - allow_joins (bool): Whether to include placeholder joins.
    - llm_config (Optional[LLMConfig]): LLM configuration for generating descriptions.

    Returns:
    - The semantic model (semantic_model_pb2.SemanticModel).

    This function fetches metadata for the specified tables, performs schema validation, extracts key information,
    enriches metadata from the Snowflake database, and constructs a semantic model in protobuf format.
    It handles different databases and schemas within the same account by creating unique Snowflake connections as needed.

    Raises:
    - AssertionError: If no valid tables are found in the specified schema.
    """

    # For FQN tables, create a new snowflake connection per table in case the db/schema is different.
    table_objects = []
    unique_database_schema: List[str] = []
    for table in base_tables:
        # Verify this is a valid FQN table. For now, we check that the table follows the following format.
        # {database}.{schema}.{table}
        fqn_table = create_fqn_table(table)
        fqn_databse_schema = f"{fqn_table.database}.{fqn_table.schema_name}"

        if fqn_databse_schema not in unique_database_schema:
            unique_database_schema.append(fqn_databse_schema)

        logger.info(f"Pulling column information from {fqn_table}")
        valid_schemas_tables_columns_df = get_valid_schemas_tables_columns_df(
            conn=conn,
            db_name=fqn_table.database,
            table_schema=fqn_table.schema_name,
            table_names=[fqn_table.table],
        )
        assert not valid_schemas_tables_columns_df.empty

        # get the valid columns for this table.
        valid_columns_df_this_table = valid_schemas_tables_columns_df[
            valid_schemas_tables_columns_df["TABLE_NAME"] == fqn_table.table
        ]

        raw_table = get_table_representation(
            conn=conn,
            schema_name=fqn_databse_schema,  # Fully-qualified schema
            table_name=fqn_table.table,  # Non-qualified table name
            table_index=0,
            ndv_per_column=n_sample_values,  # number of sample values to pull per column.
            columns_df=valid_columns_df_this_table,
            max_workers=1,
        )
        table_object = _raw_table_to_semantic_context_table(
            database=fqn_table.database,
            schema=fqn_table.schema_name,
            raw_table=raw_table,
            conn=conn,
            llm_config=llm_config,
        )
        table_objects.append(table_object)

    # Create context for semantic model description
    model_context = f"Semantic Model: {semantic_model_name}\nTables: {', '.join(base_tables)}"
    
    # Generate relationships from foreign keys if allow_joins is True
    relationships = None
    if allow_joins:
        logger.info("Detecting relationships from foreign keys")
        relationships = _generate_relationships_from_foreign_keys(table_objects, conn)
    
    context = semantic_model_pb2.SemanticModel(
        name=semantic_model_name,
        tables=table_objects,
        relationships=relationships,
    )
    return context


def comment_out_section(yaml_str: str, section_name: str) -> str:
    """
    Comments out all lines in the specified section of a YAML string.

    Parameters:
    - yaml_str (str): The YAML string to process.
    - section_name (str): The name of the section to comment out.

    Returns:
    - str: The modified YAML string with the specified section commented out.
    """
    updated_yaml = []
    lines = yaml_str.split("\n")
    in_section = False
    section_indent_level = 0

    for line in lines:
        stripped_line = line.strip()

        # When we find a section with the provided name, we can start commenting out lines.
        if stripped_line.startswith(f"{section_name}:"):
            in_section = True
            section_indent_level = len(line) - len(line.lstrip())
            comment_indent = " " * section_indent_level
            updated_yaml.append(f"{comment_indent}# {line.strip()}")
            continue

        # Since this method parses a raw YAML string, we track whether we're in the section by the indentation level.
        # This is a pretty rough heuristic.
        current_indent_level = len(line) - len(line.lstrip())
        if (
            in_section
            and current_indent_level <= section_indent_level
            and stripped_line
        ):
            in_section = False

        # Comment out the field and its subsections, preserving the indentation level.
        if in_section and line.strip():
            comment_indent = " " * current_indent_level
            updated_yaml.append(f"{comment_indent}# {line.strip()}")
        else:
            updated_yaml.append(line)

    return "\n".join(updated_yaml)


def append_comment_to_placeholders(yaml_str: str) -> str:
    """
    Finds all instances of a specified placeholder in a YAML string and appends a given text to these placeholders.
    This is the homework to fill out after your yaml is generated.

    Parameters:
    - yaml_str (str): The YAML string to process.

    Returns:
    - str: The modified YAML string with appended text to placeholders.
    """
    updated_yaml = []
    # Split the string into lines to process each line individually
    lines = yaml_str.split("\n")

    for line in lines:
        # Check if the placeholder is in the current line.
        # Strip the last quote to match.
        if line.rstrip("'").endswith(_PLACEHOLDER_COMMENT):
            # Replace the _PLACEHOLDER_COMMENT with itself plus the append_text
            updated_line = line + _FILL_OUT_TOKEN
            updated_yaml.append(updated_line)
        elif line.rstrip("'").endswith(AUTOGEN_TOKEN):
            updated_line = line + _AUTOGEN_COMMENT_TOKEN
            updated_yaml.append(updated_line)
        # Add comments to specific fields in certain sections.
        elif line.lstrip().startswith("join_type"):
            updated_line = line + _FILL_OUT_TOKEN + "  supported: inner, left_outer"
            updated_yaml.append(updated_line)
        elif line.lstrip().startswith("relationship_type"):
            updated_line = (
                line + _FILL_OUT_TOKEN + " supported: many_to_one, one_to_one"
            )
            updated_yaml.append(updated_line)
        else:
            updated_yaml.append(line)

    # Join the lines back together into a single string
    return "\n".join(updated_yaml)


def _to_snake_case(s: str) -> str:
    """
    Convert a string into snake case.

    Parameters:
    s (str): The string to convert.

    Returns:
    str: The snake case version of the string.
    """
    # Replace common delimiters with spaces
    s = s.replace("-", " ").replace("_", " ")

    words = s.split(" ")

    # Convert each word to lowercase and join with underscores
    snake_case_str = "_".join([word.lower() for word in words if word]).strip()

    return snake_case_str


def generate_base_semantic_model_from_snowflake(
    base_tables: List[str],
    conn: SnowflakeConnection,
    semantic_model_name: str,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    output_yaml_path: Optional[str] = None,
) -> None:
    """
    Generates a base semantic context from specified Snowflake tables and exports it to a YAML file.

    Parameters:
        base_tables : Fully qualified names of Snowflake tables to include in the semantic context.
        conn: SnowflakeConnection to reuse.
        snowflake_account: Identifier of the Snowflake account.
        semantic_model_name: The human readable model name. This should be semantically meaningful to an organization.
        output_yaml_path: Path for the output YAML file. If None, defaults to 'semantic_model_toolkit/output_models/YYYYMMDDHHMMSS_<semantic_model_name>.yaml'.
        n_sample_values: The number of sample values to populate for all columns.

    Returns:
        None. Writes the semantic context to a YAML file.
    """
    formatted_datetime = datetime.now().strftime("%Y%m%d%H%M%S")
    if not output_yaml_path:
        file_name = f"{formatted_datetime}_{_to_snake_case(semantic_model_name)}.yaml"
        if os.path.exists("semantic_model_toolkit/output_models"):
            write_path = f"semantic_model_toolkit/output_models/{file_name}"
        else:
            write_path = f"./{file_name}"
    else:  # Assume user gives correct path.
        write_path = output_yaml_path

    yaml_str = generate_model_str_from_snowflake(
        base_tables,
        n_sample_values=n_sample_values if n_sample_values > 0 else 1,
        semantic_model_name=semantic_model_name,
        conn=conn,
    )

    with open(write_path, "w") as f:
        # Remove auto-generated comment
        f.write(yaml_str)

    logger.info(f"Semantic model saved to {write_path}")

    return None


def generate_model_str_from_snowflake(
    base_tables: List[str],
    semantic_model_name: str,
    conn: SnowflakeConnection,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    allow_joins: bool = True,
    llm_config: Optional[LLMConfig] = None,
) -> str:
    """
    Generate a semantic model YAML string from Snowflake tables.
    
    Args:
        base_tables: List of fully qualified table names
        semantic_model_name: Name for the semantic model
        conn: Snowflake connection
        n_sample_values: Number of sample values to include per column
        allow_joins: Whether to include placeholder joins
        llm_config: Configuration for LLM description generation
        
    Returns:
        Generated semantic model as a YAML string
    """
    # Generate semantic context from Snowflake
    semantic_context = raw_schema_to_semantic_context(
        base_tables=base_tables,
        semantic_model_name=semantic_model_name,
        conn=conn,
        n_sample_values=n_sample_values,
        allow_joins=allow_joins,
        llm_config=llm_config,
    )
    
    # Convert to YAML
    yaml_str = proto_utils.semantic_model_to_yaml(semantic_context)
    
    return yaml_str
